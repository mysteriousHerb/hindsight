---
sidebar_position: 3
---

# Reflect

Generate disposition-aware responses using an **agentic loop** that dynamically gathers information from your memory bank.

When you call **reflect**, Hindsight launches an agent that:
1. **Explores** your memory bank using tools (lookup, recall, expand)
2. **Applies** the bank's disposition traits to shape the reasoning style
3. **Learns** new insights as mental models when warranted
4. **Generates** a contextual answer grounded in the retrieved facts

The agent iterates up to 10 times, deciding what information to retrieve at each step.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeSnippet from '@site/src/components/CodeSnippet';

{/* Import raw source files */}
import reflectPy from '!!raw-loader!@site/examples/api/reflect.py';
import reflectMjs from '!!raw-loader!@site/examples/api/reflect.mjs';
import reflectSh from '!!raw-loader!@site/examples/api/reflect.sh';

:::info How Reflect Works
Learn about the agentic reflect and disposition-driven reasoning in the [Reflect Architecture](/developer/reflect) guide.
:::

:::tip Prerequisites
Make sure you've completed the [Quick Start](./quickstart) to install the client and start the server.
:::

## Basic Usage

<Tabs>
<TabItem value="python" label="Python">
<CodeSnippet code={reflectPy} section="reflect-basic" language="python" />
</TabItem>
<TabItem value="node" label="Node.js">
<CodeSnippet code={reflectMjs} section="reflect-basic" language="javascript" />
</TabItem>
<TabItem value="cli" label="CLI">
<CodeSnippet code={reflectSh} section="reflect-basic" language="bash" />
</TabItem>
</Tabs>

## Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `query` | string | required | Question or prompt |
| `max_tokens` | int | 4096 | Maximum tokens for the response |
| `tags` | array | None | Filter memories by tags |
| `tags_match` | string | "any" | How to match tags: "any", "all", "any_strict", or "all_strict" |
| `include` | object | None | Options for including debug traces (see below) |

### Include Options

| Field | Type | Description |
|-------|------|-------------|
| `include.facts` | object | Include facts that support the answer. Set to `{}` to enable. |
| `include.tool_calls` | object | Include tool call traces. Set to `{}` for full trace. |
| `include.tool_calls.output` | bool | Include tool outputs (default: true). Set to `false` for smaller payload. |

### Response Fields

| Field | Type | Description |
|-------|------|-------------|
| `text` | string | The generated answer text |
| `based_on` | array | Facts used (when `include.facts` is set) |
| `tool_calls` | array | Tool call trace (when `include.tool_calls` is set) |
| `llm_calls` | array | LLM call trace (when `include.tool_calls` is set) |
| `mental_models` | array | Mental models accessed (when `include.facts` is set) |

<Tabs>
<TabItem value="python" label="Python">
<CodeSnippet code={reflectPy} section="reflect-with-params" language="python" />
</TabItem>
<TabItem value="node" label="Node.js">
<CodeSnippet code={reflectMjs} section="reflect-with-params" language="javascript" />
</TabItem>
</Tabs>

## The Agent Loop

The reflect agent iteratively uses tools to answer your query:

| Tool | Purpose |
|------|---------|
| **lookup(model_id?)** | Get mental models — without ID lists all, with ID gets full details |
| **recall(query, max_tokens?)** | Search facts using semantic + temporal retrieval (default 4096 tokens) |
| **expand(memory_id, depth)** | Get surrounding context — depth is "chunk" or "document" |
| **learn(name, description, entity_id?)** | Create a mental model placeholder to track a topic |
| **done(sections)** | Signal completion with structured answer including fact/model citations |

**Example agent flow:**
```
Query: "Should we hire Alice for the ML team?"

Iteration 1:
  lookup() → Lists all mental models
  recall("Alice background experience", max_tokens=8000) → Gets relevant facts

Iteration 2:
  expand(fact_id, "document") → Gets full context on a key fact
  learn(name="Alice", description="Senior ML engineer candidate") → Creates placeholder

Iteration 3:
  done(sections=[{title: "", text: "Based on Alice's ML expertise...", fact_ids: [...]}])
```

## Mental Model Learning

During reflection, the agent may create new **mental models** when it discovers insights worth remembering. These become available for future queries.

```python
# After reflect(), check what was learned
response = client.reflect(
    bank_id="my-bank",
    query="What do you know about the React migration?"
)

if response.mental_models_created:
    print(f"Learned: {response.mental_models_created}")
    # e.g., ["react-migration"]
```

Learn more about mental models in the [Mental Models](/developer/mental-models) guide.

## Tag Filtering

Filter which memories the agent can access using tags:

<Tabs>
<TabItem value="python" label="Python">

```python
# Only consider engineering-related memories
response = client.reflect(
    bank_id="my-bank",
    query="What are the key blockers for Q1?",
    tags=["engineering", "q1-planning"],
    tags_match="any"  # Match any of these tags
)
```

</TabItem>
<TabItem value="cli" label="CLI">

```bash
hindsight memory reflect my-bank "What are the key blockers?" \
  --tags engineering,q1-planning \
  --tags-match any
```

</TabItem>
</Tabs>

| tags_match | Behavior |
|------------|----------|
| `any` | Match memories with ANY of the specified tags (OR) |
| `all` | Match memories with ALL of the specified tags (AND) |
| `any_strict` | Match memories that have ONLY specified tags, matching any |
| `all_strict` | Match memories that have ONLY specified tags, matching all |

## Disposition Influence

The bank's disposition affects reflect responses:

| Trait | Low (1) | High (5) |
|-------|---------|----------|
| **Skepticism** | Trusting, accepts claims | Questions and doubts claims |
| **Literalism** | Flexible interpretation | Exact, literal interpretation |
| **Empathy** | Detached, fact-focused | Considers emotional context |

<Tabs>
<TabItem value="python" label="Python">
<CodeSnippet code={reflectPy} section="reflect-disposition" language="python" />
</TabItem>
<TabItem value="node" label="Node.js">
<CodeSnippet code={reflectMjs} section="reflect-disposition" language="javascript" />
</TabItem>
</Tabs>

## Reasoning Behavior

The reflect agent is designed to **synthesize and reason** from retrieved data:

1. **Synthesize**: Connect related facts to form a complete picture
2. **Infer**: Make reasonable inferences when the exact answer isn't stated
3. **Always search**: Must call `recall()` before saying "I don't have information"
4. **Be helpful**: Use related information to give the best possible answer
5. **No fabrication**: Don't completely invent facts with no basis in retrieved data

:::tip
The agent is a thoughtful interpreter, not just a literal fact repeater. It will make reasonable inferences from the data to provide helpful answers.
:::

## HTTP API

**Endpoint:** `POST /v1/default/banks/{bank_id}/reflect`

**Basic Request:**
```json
{
  "query": "Should we hire Alice for the ML team?",
  "max_tokens": 4096,
  "tags": ["engineering"],
  "tags_match": "any"
}
```

**Request with Debug Traces:**
```json
{
  "query": "What do you know about the React migration?",
  "include": {
    "tool_calls": {},
    "facts": {}
  }
}
```

**Basic Response:**
```json
{
  "text": "## Overview\nBased on Alice's ML expertise...\n\n## Key Qualifications\nShe has 5 years of experience..."
}
```

**Response with Traces:**
```json
{
  "text": "Based on Alice's ML expertise...",
  "tool_calls": [
    {"tool": "lookup", "input": {}, "output": {"models": [...]}, "duration_ms": 45},
    {"tool": "recall", "input": {"query": "Alice"}, "output": {"facts": [...]}, "duration_ms": 120}
  ],
  "llm_calls": [
    {"scope": "agent_1", "duration_ms": 450},
    {"scope": "final", "duration_ms": 320}
  ],
  "mental_models": [
    {"id": "alice", "name": "Alice Chen", "subtype": "emergent", "description": "Senior ML engineer"}
  ],
  "based_on": [
    {"id": "uuid-1", "text": "Alice joined from Google in 2023", "type": "world"}
  ]
}
```

## Next Steps

- [**Reflect Architecture**](/developer/reflect) — Deep dive into how agentic reflect works
- [**Mental Models**](/developer/mental-models) — Understanding synthesized knowledge
- [**Recall**](./recall) — Direct fact retrieval without reasoning
